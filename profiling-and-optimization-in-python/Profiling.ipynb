{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Profiling.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"cells":[{"cell_type":"markdown","metadata":{"id":"m47-T8JxBcOe"},"source":["# Profiling\n","\n","## Why profile?\n","\n","Profiling is the act of finding the demands placed on your computer when a piece of code is run and, preferably, localising the demands to specific sections of code. This is important when optimising as it allows you to find which parts of your code contribute to the demand you want to reduce and then target your optimisation efforts there.\n","\n","For instance, if ```function_1``` takes 99% of the runtime of your code and ```function2``` takes the rest, there is no point in optimising ```function2``` as even reducing the runtime to zero for that function would not produce a noticable impact on the runtime of the code as a whole. Instead, optimising ```function1``` should be the priority.\n","\n","For this reason, when optimising a code, the first step is to profile it. This means the time you spend optimising your code will have most impact.\n","\n","## The Effects of Profiling\n","\n","One important note when using any profiling technique is that it can impact the run-time and memory usage of the code. This is because tracking various aspects of performance can actually impact performance as more information needs to be collected. As such, it's sometimes necessary to run a profiler on a reduced scale problem so the code runs in a reasonable time.\n","\n","## Profiling Tools\n","\n","There are a number of different profiling tools available for use in Python. In this notebook, we will look at a sample of them - favouring ones which are simple to implement in a Jupyter Notebook. However, this list is far from exhaustive.\n","\n","The examples in this section relate to running this notebook in Colab. To run this notebook in Anaconda locally a different approach may be required in some cases. Many of the commands are also available from the command line when profiling a ```.py``` file. Again, a different approach will be required in this usage case."]},{"cell_type":"markdown","metadata":{"id":"kFnTlggDMezd"},"source":["## Profiling Effectively\n","\n","### Profiling Runs\n","\n","In many cases, when profiling and optimising a large and complex code, it is not possible to perform full-scale executions of the code. This may be because, for example, the code is designed to run on HPC resources but you're profiling on your desktop. Or it may be that you expect the codes to take hours or days to run and you want to make your profiling and optimising cycle shorter.\n","\n","In addition, it may be that a code may be used in a number of different ways, such as performing many different kinds of simulation or analysing many different kinds of data set. It may be that the code can be run with a multitude of different options that will invoke different parts of the code.\n","\n","These considerations may mean that simply \"profiling the code\" is not a striaghtforward affair. Which use cases do you profile? It may be that you need to develop a suite of cases to profile to get a measure of how the code performs in different scenarios.\n","\n","### Length of Profiling Runs\n","\n","One other consideration that should always be taken into account is that a very short piece of code will be prone to relatively larger noise than a longer execution of code. For instance, if profile the same bit of code twice, you may get noticably different runtimes if on one occassion your computational resources were split due to another task taking place on your computer (for instance, if a software update is taking place). This means that a code needs to run for at least a few seconds for profiling to produce meaningful results.\n","\n","One solution if you're working with a shorter piece of code is to run it many times and look at the total time taken. However, if this is the case, you need to make sure that the overhead of looping over the piece of code to be profiled is not taking up too large a portion of the run-time.\n","\n","### Profiling Effective - Takeaway\n","\n","There isn't a simple answer to ensure you get an effective profile of your code as it may be very problem-specific. You should try to think about what your code is doing and what you want to profile carefully and develop a strategy. Then repeat the profiling a few times to ensure the results remain the same."]},{"cell_type":"markdown","metadata":{"id":"uiLBPKBsHejk"},"source":["## Run-time Profiling\n","\n","Profiling the run-time of the code is probably one of the most important parts of profiling. In many cases, the amount of time the code takes to run is the most limiting factor when deciding if a particular simulation can be carried out on the available computational resources in a reasonable amount of time. There are several useful bits of useful information we can gather.\n","\n","### Profiling By Function\n","Profiling by function gives us a high-level idea of how often functions are called and how long those calls last. One way to do this is to import the ```cProfile``` module and run a function using the ```cProfile.run()``` function, providing a string argument which is the command used to invoke the function."]},{"cell_type":"code","metadata":{"id":"4LYP3nG9NqdE"},"source":["import cProfile\n","\n","def is_even(value):\n","  if value%2 == 0:\n","    return True\n","  else:\n","    return False\n","\n","def halve(value):\n","  return(value / 2)\n","\n","def function_to_test(upper_value):\n","  result = 0\n","  for i in range(int(upper_value) +1):\n","    if is_even(i):\n","      result=result + halve(i)\n","\n","  print(result)\n","\n","cProfile.run('function_to_test(1e7)')\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zN9diO3JslR-"},"source":["The results from this show how many times each function is called, the total spent within that function (excluding time spent in functions called from this function), the amount of time spent on that function per call (this may be rounded to zero if the run-time is very short), the time spent in that function and other functions called from it and the time spent in that function and other functions called from it per call.\n","\n","There will normally be a number of functions which are not functions you are written or will be aware of having called. These are often called for reasons internal to how Python is executed and are normally not very consequential in terms of run-time and can often be ignored."]},{"cell_type":"markdown","metadata":{"id":"uy2_iLJlzyxw"},"source":["### Call Graphs\n","Knowing in what context a function is called is very useful for understanding the performance of a code through profiling. This allows you to understand the number of the times different functions call each other and to understand potentially complicated webs of funciton calls and understand why the code is spending the time it does in each function.\n","\n","Sometimes, this information will be relatively straightforward due to the structure of the code. In more complex codes, it will not always be obvious. In these cases, generating a call graph can be a useful way to obtain and visualise this information.\n","\n","One example of a Python module capable of doing this is [```pycallgraph```](http://pycallgraph.slowchop.com/en/master/index.html). An example is shown below:"]},{"cell_type":"code","metadata":{"id":"aaDMLWkP0-yJ","scrolled":true},"source":["#Install the module using pip\n","!pip install pycallgraph\n","\n","#Load the module in the notebook environment\n","%load_ext pycallgraph\n","\n","#Import the relevant modules\n","from pycallgraph import PyCallGraph\n","from pycallgraph.output import GraphvizOutput\n","\n","#Define the modules to be tested\n","def is_multiple_of_four(value):\n","  if value % 4 == 0:\n","    return True\n","  else:\n","    return False\n","\n","def halve(value):\n","  return(value / 2)\n","\n","def quarter(value):\n","  return(halve(halve(value)))\n","\n","def function_to_test(upper_value):\n","  result = 0\n","  for i in range(int(upper_value) +1):\n","    if is_multiple_of_four(i):\n","      result=result + halve(i)+quarter(i)\n","\n","  print(\"The result is \", result)\n","\n","#Call the function the call graph is to be generated for with an instance of the PyCallGraph class\n","with PyCallGraph(output=GraphvizOutput()):\n","    function_to_test(1e5)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1SOrXFhq4yKC"},"source":["This will have created a file named ```pycallgraph.png```. In Colab, this will be in the local filespace, which can be accessed by opening the left tab (look for the chevron on the left sie of the screen) and selecting \"Files\". \n","\n","The resultant file shows the hierarchy of which functions have called which other functions."]},{"cell_type":"markdown","metadata":{"id":"W0LRJre2luKm"},"source":["### Profiling By Line\n","\n","Sometimes, knowing why a code spends a lot of time in a function doesn't provide enough granularity to know exactly which operations are costly. In these cases, there are several options for find out how much time the code spends executing each line. One example is the  [```line_profiler```](https://pypi.org/project/line-profiler/) module.\n","\n","The first two lines of the following cell install and load this module. The final line uses the line profiler to call the function ```function_to_test``` with a value of $10^6$ passed to the ```upper_value``` argument. The function ```is_even``` is specified as the function to be profiled.\n","\n","Running the below cell calls ```function_to_test``` and reports the number of times each line is executed, the length of each execuation and the total and fraction of time spent on each line of the function being profiled."]},{"cell_type":"code","metadata":{"id":"2FgjFKqOlwcu"},"source":["#Install the relevant module\n","!pip install line_profiler\n","\n","#Load the module\n","%load_ext line_profiler\n","\n","def is_even(value):\n","  if value%2 == 0:\n","    return True\n","  else:\n","    return False\n","\n","def halve(value):\n","  return(value / 2)\n","\n","def function_to_test(upper_value):\n","  result = 0\n","  for i in range(int(upper_value) +1):\n","    if is_even(i):\n","      result=result + halve(i)\n","\n","  print(\"The result is: \", result)\n","\n","#\"lprun\" invokes the line profiler\n","#We specify \"is_even\" as the function we want to profile\n","#\"function_to_test(1e6)\" specifies the command we want to be executed\n","#This allows the performance of a function to be tested in a specified context\n","%lprun -f is_even function_to_test(1e6)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MvFPtk-wAl7l"},"source":["In Colab, the results will be presented in a pop-up labeled \"Help\" in the bottom right of the screen. This displays, for each line number, the number of times it's called and the time spent on each line in total and per call. It also show the percentage of the total time spent on that line and the contents of the line. Note that all times will need to be multiplied by the \"Timer unit\" (likely 1$\\mu$s) to find the actual time taken."]},{"cell_type":"markdown","metadata":{"id":"PPs8EDfN5xqI"},"source":["### Exercise\n","\n","The code in the cell below has a particular bottleneck that determines the bulk of the runtime of the code.\n","\n","Examine the code by eye and try to understand how it works, where the bottleneck might lie and what you might do to optimise it. Don't run or change the code at this point.\n"]},{"cell_type":"code","metadata":{"id":"EyLG0ouo6lPj"},"source":["# The goal of this code is to print either \"X is is prime\" or \"X is not prime\" for every value of X between 1 and 1000\n","\n","import math\n","\n","# This method checks if \"value\" is prime and returns True if it is and False if it isn't\n","def check_prime(value):\n","  #Initially assume the value is prime\n","  prime=True\n","\n","  # Loop over the values which value may be divisible by and check if it's a multiple of any of them\n","  for i in range(2, int(math.sqrt(value))+1):\n","    if value%i==0:\n","      # If value is divisible by i, value%i will be zero, so set prime to false\n","      prime=False\n","\n","  # Now we've checked if value is divisible by any of the values it might be divisible by, return prime\n","  return(prime)\n","\n","# This function prints a message dependent on whether the given value is prime or not\n","def print_value_prime(value):\n","  if check_prime(value):\n","    print(value, \" is prime\")\n","  else:\n","    print(value, \" is not prime\")\n","\n","def print_primes(max_value):\n","# Loop over all numbers between 1 and max_value, printing if they're prime or not\n","  for i in range(1, max_value+1):\n","    print_value_prime(i)\n","\n","#Print whether values between 1 and 1000 are prime\n","print_primes(1000)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jjdISmjQB2jv"},"source":["Use ```cProfile``` and/or ```pycallgraph``` to work out which function this bottleneck lies in, then use ```line_profiler``` to work out which line(s) the bottleneck lies in. Does this match where you thought it might be by eye? Would it be worth performing the optimisations you came up with earlier?"]},{"cell_type":"code","metadata":{"cellView":"form","id":"fDhp3s5dCIzN"},"source":["#@title\n","\n","import math\n","import cProfile\n","\n","!pip install line_profiler\n","%load_ext line_profiler\n","\n","# This method checks if \"value\" is prime and returns True if it is and False if it isn't\n","def check_prime(value):\n","  #Initially assume the value is prime\n","  prime=True\n","\n","  # Loop over the values which value may be divisible by and check if it's a multiple of any of them\n","  for i in range(2, int(math.sqrt(value))+1):\n","    if value%i==0:\n","      # If value is divisible by i, value%i will be zero, so set prime to false\n","      prime=False\n","\n","  # Now we've checked if value is divisible by any of the values it might be divisible by, return prime\n","  return(prime)\n","\n","# This function prints a message dependent on whether the given value is prime or not\n","def print_value_prime(value):\n","  if check_prime(value):\n","    print(value, \" is prime\")\n","  else:\n","    print(value, \" is not prime\")\n","\n","def print_primes(max_value):\n","# Loop over all numbers between 1 and max_value, printing if they're prime or not\n","  for i in range(1, max_value+1):\n","    print_value_prime(i)\n","\n","#Print whether values between 1 and 1000 are prime\n","cProfile.run(\"print_primes(1000)\")\n","#%lprun -f print_value_prime print_primes(1000)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ng3CXicOH3jM"},"source":["## Memory Profiling\n","\n"]},{"cell_type":"markdown","metadata":{"id":"1WNcsJ9fIPis"},"source":["It's also possible to profile the memory which is used by the code. The amount of memory used will change over time as the code runs and data is created and discarded. One meaningful measuerment which can be made is the difference in the amount of memory used at two different points in the program. If the first point is the start of the program then this will give a good approximation of how much memory is in use at the second point. Often it will be desirable for the second point to be when the maximum memory is in use and this may take some experimentation to find.\n","\n","One example of a memory profiler in Python is [```pympler```](https://pympler.readthedocs.io/en/latest/index.html#). An example is found below:"]},{"cell_type":"code","metadata":{"id":"mJ_uJ9MSgnfy"},"source":["#Install pympler using pip\n","!pip install pympler\n","\n","#Import the relevant parts of pympler\n","from pympler import summary, muppy\n","\n","#Get the first measure of the objects in memory\n","sum1 = muppy.get_objects()\n","\n","#Create some new objects in memory\n","list1=[]\n","\n","for i in range(100):\n","  new_dict = {}\n","  for j in range(100):\n","    new_dict[j] = [1000+i*100+j]\n","  \n","  list1.append(new_dict)\n","\n","#Get the second measure of the objects in memory\n","sum2 = muppy.get_objects()\n","\n","#Print a comparison the two measure of the objects in memory\n","summary.print_(summary.get_diff(summary.summarize(sum1), summary.summarize(sum2)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3T9KoAYfIO2L"},"source":["The summry printed shows the change in the number of different types of object stored in memory and the change in the amount of memory allocated to each type of object. In the example above, we have created a list containing 100 dictionaries, each containing 100 lists with each list containing one unique integer. Note that there are also a small number of other values created. These may be created due to processes happening \"under the hood\". Given their small contribution to the total number of values stored, they're not very significant."]}]}