{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning with Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_files\n",
    "\n",
    "reviews_train = load_files(\"../assets/imdb/train/\")\n",
    "text_train, y_train = reviews_train.data, reviews_train.target\n",
    "text_train = [doc.replace(b\"<br />\", b\" \") for doc in text_train]\n",
    "\n",
    "reviews_test = load_files(\"../assets/imdb/test/\")\n",
    "text_test, y_test = reviews_test.data, reviews_test.target\n",
    "text_test = [doc.replace(b\"<br />\", b\" \") for doc in text_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2 Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The task\n",
    "\n",
    "Our first machine learning task will be a *binary classification*, trying to make a predictor for the class label (i.e. positive or negative review) based on the text. This is a form of [*sentiment analysis*](https://towardsdatascience.com/sentiment-analysis-concept-analysis-and-applications-6c94d6f58c17).\n",
    "\n",
    "Unlike our previous examples using structured data, we do not yet have any features that can be used to a learning algorithm. Finding a useful data representation is therefore a key component of NLP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words\n",
    "\n",
    "A very simple but usually quite effective approach is the so-called *bag of words*.\n",
    "\n",
    "Here, we __discard the information contained in the document structure and the order of the words in each sentence,__ and just represent each document as a frequency table showing how often each word occurs therein."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The three stages are\n",
    "\n",
    "* *Tokenization* - break each document into a list of words.\n",
    "* *Vocabulary building* - collect all the words found in the corpus and sort them in alphabetical order.\n",
    "* *Encoding* - for each document, create a frequency table over the vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a simple example on two short documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bards_words = [\n",
    "    \"The fool doth think he is wise,\",\n",
    "    \"but the wise man knows himself to be a fool\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>CountVectorizer()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "CountVectorizer()"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect = CountVectorizer()\n",
    "vect.fit(bards_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 13\n",
      "Vocabulary content:\n",
      " {'the': 9, 'fool': 3, 'doth': 2, 'think': 10, 'he': 4, 'is': 6, 'wise': 12, 'but': 1, 'man': 8, 'knows': 7, 'himself': 5, 'to': 11, 'be': 0}\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocabulary size: {}\".format(len(vect.vocabulary_)))\n",
    "print(\"Vocabulary content:\\n {}\".format(vect.vocabulary_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vectorizer is case-aware, so it knows that \"The\" and \"the\" are the same word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bag_of_words: <2x13 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 16 stored elements in Compressed Sparse Row format>\n"
     ]
    }
   ],
   "source": [
    "bag_of_words = vect.transform(bards_words)\n",
    "print(\"bag_of_words: {}\".format(repr(bag_of_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sparse matrix format is much more memory-efficient for data where we expect many zeros.\n",
    "\n",
    "For inspection, we can use `toarray()` to change this back into a \"dense\" numpy array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dense representation of bag_of_words:\n",
      "[[0 0 1 1 1 0 1 0 0 1 1 0 1]\n",
      " [1 1 0 1 0 1 0 1 1 1 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Dense representation of bag_of_words:\\n{}\".format(\n",
    "    bag_of_words.toarray()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the words with index 3 (\"fool\") ,9 (\"the\") and 12 (\"wise\") appear in both documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try the same approach with the IMDB data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:\n",
      "<25000x74849 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 3431196 stored elements in Compressed Sparse Row format>\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer().fit(text_train)\n",
    "X_train = vect.transform(text_train)\n",
    "print(\"X_train:\\n{}\".format(repr(X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 74849\n",
      "First 20 features:\n",
      "['00' '000' '0000000000001' '00001' '00015' '000s' '001' '003830' '006'\n",
      " '007' '0079' '0080' '0083' '0093638' '00am' '00pm' '00s' '01' '01pm' '02']\n",
      "Features 20010 to 20030:\n",
      "['dratted' 'draub' 'draught' 'draughts' 'draughtswoman' 'draw' 'drawback'\n",
      " 'drawbacks' 'drawer' 'drawers' 'drawing' 'drawings' 'drawl' 'drawled'\n",
      " 'drawling' 'drawn' 'draws' 'draza' 'dre' 'drea']\n",
      "Every 2000th feature:\n",
      "['00' 'aesir' 'aquarian' 'barking' 'blustering' 'bête' 'chicanery'\n",
      " 'condensing' 'cunning' 'detox' 'draper' 'enshrined' 'favorit' 'freezer'\n",
      " 'goldman' 'hasan' 'huitieme' 'intelligible' 'kantrowitz' 'lawful' 'maars'\n",
      " 'megalunged' 'mostey' 'norrland' 'padilla' 'pincher' 'promisingly'\n",
      " 'receptionist' 'rivals' 'schnaas' 'shunning' 'sparse' 'subset'\n",
      " 'temptations' 'treatises' 'unproven' 'walkman' 'xylophonist']\n"
     ]
    }
   ],
   "source": [
    "feature_names = vect.get_feature_names_out()\n",
    "print(\"Number of features: {}\".format(len(feature_names)))\n",
    "print(\"First 20 features:\\n{}\".format(feature_names[:20]))\n",
    "print(\"Features 20010 to 20030:\\n{}\".format(feature_names[20010:20030]))\n",
    "print(\"Every 2000th feature:\\n{}\".format(feature_names[::2000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the definition of \"word\" is currently very basic, just any text that is separable by white space or punctuation, so numbers, alternative word forms and typos all appear as different features. \n",
    "\n",
    "We can now try a classifier. Using logistic regression on the defined features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean cross-validation accuracy: 0.88\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "scores = cross_val_score(LogisticRegression(max_iter=100000), X_train, y_train, cv=5)\n",
    "print(\"Mean cross-validation accuracy: {:.2f}\".format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad for such a simple model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One simple improvement is to restrict the allowed features to words that appear at least a minimum number of times in the corpus. This will help to weed out typos and other uninformative text, and also reduce the size of the feature space, which is helpful in itself. Here we will set `min_df=5`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train with min_df: <25000x27271 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 3354014 stored elements in Compressed Sparse Row format>\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer(min_df=5).fit(text_train)\n",
    "X_train = vect.transform(text_train)\n",
    "print(\"X_train with min_df: {}\".format(repr(X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 50 features:\n",
      "['00' '000' '007' '00s' '01' '02' '03' '04' '05' '06' '07' '08' '09' '10'\n",
      " '100' '1000' '100th' '101' '102' '103' '104' '105' '107' '108' '10s'\n",
      " '10th' '11' '110' '112' '116' '117' '11th' '12' '120' '12th' '13' '135'\n",
      " '13th' '14' '140' '14th' '15' '150' '15th' '16' '160' '1600' '16mm' '16s'\n",
      " '16th']\n",
      "Features 20010 to 20030:\n",
      "['repentance' 'repercussions' 'repertoire' 'repetition' 'repetitions'\n",
      " 'repetitious' 'repetitive' 'rephrase' 'replace' 'replaced' 'replacement'\n",
      " 'replaces' 'replacing' 'replay' 'replayable' 'replayed' 'replaying'\n",
      " 'replays' 'replete' 'replica']\n",
      "Every 700th feature:\n",
      "['00' 'affections' 'appropriately' 'barbra' 'blurbs' 'butchered' 'cheese'\n",
      " 'commitment' 'courts' 'deconstructed' 'disgraceful' 'dvds' 'eschews'\n",
      " 'fell' 'freezer' 'goriest' 'hauser' 'hungary' 'insinuate' 'juggle'\n",
      " 'leering' 'maelstrom' 'messiah' 'music' 'occasional' 'parking'\n",
      " 'pleasantville' 'pronunciation' 'recipient' 'reviews' 'sas' 'shea'\n",
      " 'sneers' 'steiger' 'swastika' 'thrusting' 'tvs' 'vampyre' 'westerns']\n"
     ]
    }
   ],
   "source": [
    "feature_names = vect.get_feature_names_out()\n",
    "\n",
    "print(\"First 50 features:\\n{}\".format(feature_names[:50]))\n",
    "print(\"Features 20010 to 20030:\\n{}\".format(feature_names[20010:20030]))\n",
    "print(\"Every 700th feature:\\n{}\".format(feature_names[::700]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean cross-validation accuracy: 0.88\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "scores = cross_val_score(LogisticRegression(max_iter=100000), X_train, y_train, cv=5)\n",
    "print(\"Mean cross-validation accuracy: {:.2f}\".format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cross-validation score is essentially unchanged, but the number of features has reduced to 1/3 of the original number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: 0.86\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(max_iter=100000)\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "X_test = vect.transform(text_test)\n",
    "\n",
    "print(\"Test score: {:.2f}\".format(lr.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords\n",
    "\n",
    "Sometimes we have a list of words that we do not want to use as features, for example because they do not add any information. We can eliminate these using the `stop_words` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stop words: 318\n",
      "Every 10th stopword:\n",
      "['hasnt', 'then', 'six', 'while', 'up', 'our', 'until', 'whence', 'nobody', 'may', 'seems', 'mill', 'whether', 'forty', 'made', 'less', 'by', 'down', 'nine', 'too', 'yourself', 'anywhere', 'anyone', 'no', 'before', 'but', 'you', 'onto', 'thereby', 'in', 'any', 'last']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "print(\"Number of stop words: {}\".format(len(ENGLISH_STOP_WORDS)))\n",
    "print(\"Every 10th stopword:\\n{}\".format(list(ENGLISH_STOP_WORDS)[::10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train with stop words:\n",
      "<25000x26966 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 2149958 stored elements in Compressed Sparse Row format>\n"
     ]
    }
   ],
   "source": [
    "# Specifying stop_words=\"english\" uses the built-in list.\n",
    "# We could also augment it and pass our own.\n",
    "vect = CountVectorizer(min_df=5, stop_words=\"english\").fit(text_train)\n",
    "X_train = vect.transform(text_train)\n",
    "print(\"X_train with stop words:\\n{}\".format(repr(X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean cross-validation accuracy: 0.88\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(LogisticRegression(max_iter=100000), X_train, y_train, cv=5)\n",
    "print(\"Mean cross-validation accuracy: {:.2f}\".format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this fixed list has not improved our model. However, if the dataset were much smaller then we may find that the use of stopwords would help to focus the model on more informative words.\n",
    "\n",
    "A simple corpus-specific option is to use the `max_df` argument to eliminate words that appear very frequently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rescaling with *tf-idf*\n",
    "\n",
    "We can try to use the corpus itself to determine feature importance. One common approach is *term frequency-inverse document frequency* (tf-idf).\n",
    "\n",
    "tf-idf for a word *w* in a document *d* is given by"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\\begin{equation*}\n",
    "\\text{tfidf}(w, d) = \\text{tf} \\log\\big(\\frac{N + 1}{N_w + 1}\\big) + 1\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where \n",
    "\n",
    "**tf** is the number of times the word appears in document *d*.\n",
    "\n",
    "*N* is the total number of documents in the training set.\n",
    "\n",
    "*Nw* is the number of documents in the training set that contain *w*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean cross-validation accuracy: 0.87\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vect = TfidfVectorizer(min_df=5, norm=None).fit(text_train)\n",
    "X_train = vect.transform(text_train)\n",
    "\n",
    "scores = cross_val_score(LogisticRegression(max_iter=100000), X_train, y_train, cv=5)\n",
    "print(\"Mean cross-validation accuracy: {:.2f}\".format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No improvement in performance... But organising the features in this way can make the model more interpretable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features with lowest tfidf:\n",
      "['poignant' 'disagree' 'instantly' 'importantly' 'lacked' 'occurred'\n",
      " 'currently' 'altogether' 'nearby' 'undoubtedly' 'directs' 'fond'\n",
      " 'stinker' 'avoided' 'emphasis' 'commented' 'disappoint' 'realizing'\n",
      " 'downhill' 'inane']\n",
      "\n",
      "Features with highest tfidf: \n",
      "['coop' 'homer' 'dillinger' 'hackenstein' 'gadget' 'taker' 'macarthur'\n",
      " 'vargas' 'jesse' 'basket' 'dominick' 'the' 'victor' 'bridget' 'victoria'\n",
      " 'khouri' 'zizek' 'rob' 'timon' 'titanic']\n"
     ]
    }
   ],
   "source": [
    "max_value = X_train.max(axis=0).toarray().ravel()\n",
    "sorted_by_tfidf = max_value.argsort()\n",
    "# get feature names\n",
    "feature_names = np.array(vect.get_feature_names_out())\n",
    "\n",
    "print(\"Features with lowest tfidf:\\n{}\".format(feature_names[sorted_by_tfidf[:20]]))\n",
    "print()\n",
    "print(\"Features with highest tfidf: \\n{}\".format(feature_names[sorted_by_tfidf[-20:]]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features with low inverse document frequency are the most commonly occuring words, which presumably have low information content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features with lowest idf:\n",
      "['the' 'and' 'of' 'to' 'this' 'is' 'it' 'in' 'that' 'but' 'for' 'with'\n",
      " 'was' 'as' 'on' 'movie' 'not' 'have' 'one' 'be' 'film' 'are' 'you' 'all'\n",
      " 'at' 'an' 'by' 'so' 'from' 'like' 'who' 'they' 'there' 'if' 'his' 'out'\n",
      " 'just' 'about' 'he' 'or' 'has' 'what' 'some' 'good' 'can' 'more' 'when'\n",
      " 'time' 'up' 'very' 'even' 'only' 'no' 'would' 'my' 'see' 'really' 'story'\n",
      " 'which' 'well' 'had' 'me' 'than' 'much' 'their' 'get' 'were' 'other'\n",
      " 'been' 'do' 'most' 'don' 'her' 'also' 'into' 'first' 'made' 'how' 'great'\n",
      " 'because' 'will' 'people' 'make' 'way' 'could' 'we' 'bad' 'after' 'any'\n",
      " 'too' 'then' 'them' 'she' 'watch' 'think' 'acting' 'movies' 'seen' 'its'\n",
      " 'him']\n"
     ]
    }
   ],
   "source": [
    "sorted_by_idf = np.argsort(vect.idf_)\n",
    "print(\"Features with lowest idf:\\n{}\".format(feature_names[sorted_by_idf[:100]]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigating the model\n",
    "\n",
    "Because the features in *bag of words* are just natural language terms, the resulting models can be highly interpretable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.1651748 , -0.0304902 , -0.11180878, ...,  0.18723533,\n",
       "       -0.00373665, -0.10921337])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.coef_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs = pd.Series(lr.coef_[0], vect.get_feature_names_out() )\n",
    "coefs = coefs.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "refreshing      1.676745\n",
       "erotic          1.380814\n",
       "wonderfully     1.368901\n",
       "carrey          1.363813\n",
       "funniest        1.327075\n",
       "surprisingly    1.304696\n",
       "flawless        1.296816\n",
       "excellent       1.291702\n",
       "vengeance       1.290208\n",
       "appreciated     1.283186\n",
       "dtype: float64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coefs.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "laughable        -1.518168\n",
       "unfunny          -1.521221\n",
       "boring           -1.544886\n",
       "mess             -1.558348\n",
       "awful            -1.729991\n",
       "lacks            -1.801141\n",
       "poorly           -1.932531\n",
       "worst            -2.155337\n",
       "disappointment   -2.162398\n",
       "waste            -2.193415\n",
       "dtype: float64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coefs.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### n-Grams\n",
    "\n",
    "One major problem with *bag of words* is that the meaning contained in __word order__ is completely lost. Compare the sentiment of\n",
    "\n",
    "* She was happy not to be going to the party.\n",
    "* She was not happy to be going to the party.\n",
    "\n",
    "If we extend our features to __encompass two or more consecutive tokens,__ then we will have a chance to extract at least some meaning from word order. This may be very important for languages whose word boundaries are not so easily recognised as in English. \n",
    "\n",
    "This approach is called *n-grams*. We use a \"sliding window\" of a specified number of tokens.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`n=1` is just the same as *bag of words*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bards_words:\n",
      "['The fool doth think he is wise,', 'but the wise man knows himself to be a fool']\n"
     ]
    }
   ],
   "source": [
    "print(\"bards_words:\\n{}\".format(bards_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 13\n",
      "Vocabulary:\n",
      "['be' 'but' 'doth' 'fool' 'he' 'himself' 'is' 'knows' 'man' 'the' 'think'\n",
      " 'to' 'wise']\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(ngram_range=(1, 1)).fit(bards_words)\n",
    "print(\"Vocabulary size: {}\".format(len(cv.vocabulary_)))\n",
    "print(\"Vocabulary:\\n{}\".format(cv.get_feature_names_out()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With *n* fixed at 2, we get different features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 14\n",
      "Vocabulary:\n",
      "['be fool' 'but the' 'doth think' 'fool doth' 'he is' 'himself to'\n",
      " 'is wise' 'knows himself' 'man knows' 'the fool' 'the wise' 'think he'\n",
      " 'to be' 'wise man']\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(ngram_range=(2, 2)).fit(bards_words)\n",
    "print(\"Vocabulary size: {}\".format(len(cv.vocabulary_)))\n",
    "print(\"Vocabulary:\\n{}\".format(cv.get_feature_names_out()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed data (dense):\n",
      "[[0 0 1 1 1 0 1 0 0 1 0 1 0 0]\n",
      " [1 1 0 0 0 1 0 1 1 0 1 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Transformed data (dense):\\n{}\".format(cv.transform(bards_words).toarray()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can specify a range for *n*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 39\n",
      "Vocabulary:\n",
      "['be' 'be fool' 'but' 'but the' 'but the wise' 'doth' 'doth think'\n",
      " 'doth think he' 'fool' 'fool doth' 'fool doth think' 'he' 'he is'\n",
      " 'he is wise' 'himself' 'himself to' 'himself to be' 'is' 'is wise'\n",
      " 'knows' 'knows himself' 'knows himself to' 'man' 'man knows'\n",
      " 'man knows himself' 'the' 'the fool' 'the fool doth' 'the wise'\n",
      " 'the wise man' 'think' 'think he' 'think he is' 'to' 'to be' 'to be fool'\n",
      " 'wise' 'wise man' 'wise man knows']\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(ngram_range=(1, 3)).fit(bards_words)\n",
    "print(\"Vocabulary size: {}\".format(len(cv.vocabulary_)))\n",
    "print(\"Vocabulary:\\n{}\".format(cv.get_feature_names_out()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how increasing the value of *n* will cause the number of features to explode rapidly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying this to the IMDB data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 128166\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer(min_df=5, ngram_range=(2, 2)).fit(text_train)\n",
    "print(\"Vocabulary size: {}\".format(len(vect.vocabulary_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean cross-validation accuracy: 0.88\n"
     ]
    }
   ],
   "source": [
    "X_train = vect.transform(text_train)\n",
    "\n",
    "scores = cross_val_score(LogisticRegression(max_iter=100000), X_train, y_train, cv=5)\n",
    "print(\"Mean cross-validation accuracy: {:.2f}\".format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(max_iter=100000)\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "coefs = pd.Series(lr.coef_[0], vect.get_feature_names_out() )\n",
    "coefs = coefs.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "definitely worth      1.380835\n",
       "well worth            1.357000\n",
       "highly recommended    1.298359\n",
       "10 10                 1.284409\n",
       "must see              1.233707\n",
       "                        ...   \n",
       "not even             -1.177808\n",
       "than this            -1.265824\n",
       "not worth            -1.399645\n",
       "waste of             -1.755501\n",
       "the worst            -2.363117\n",
       "Length: 128166, dtype: float64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coefs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Try applying *bag of words* to the Spanish language paper reviews. \n",
    "\n",
    "Can you fit a linear regression to predict the `evaluation` score?\n",
    "\n",
    "Apply some of the variations discussed above to see if you can improve cross-validation performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# load data using Python JSON module\n",
    "with open('../assets/reviews.json','r') as f:\n",
    "    data = json.loads(f.read())\n",
    "\n",
    "reviews = pd.json_normalize(data, record_path=['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['confidence', 'evaluation', 'id', 'lan', 'orientation', 'remarks',\n",
       "       'text', 'timespan'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>confidence</th>\n",
       "      <th>evaluation</th>\n",
       "      <th>id</th>\n",
       "      <th>lan</th>\n",
       "      <th>orientation</th>\n",
       "      <th>remarks</th>\n",
       "      <th>text</th>\n",
       "      <th>timespan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>es</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>- El artículo aborda un problema contingente y...</td>\n",
       "      <td>2010-07-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>es</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>El artículo presenta recomendaciones prácticas...</td>\n",
       "      <td>2010-07-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>es</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>- El tema es muy interesante y puede ser de mu...</td>\n",
       "      <td>2010-07-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>es</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>Se explica en forma ordenada y didáctica una e...</td>\n",
       "      <td>2010-07-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>es</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2010-07-05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  confidence evaluation  id lan orientation remarks  \\\n",
       "0          4          1   1  es           0           \n",
       "1          4          1   2  es           1           \n",
       "2          5          1   3  es           1           \n",
       "3          4          2   1  es           1           \n",
       "4          4          2   2  es           0           \n",
       "\n",
       "                                                text    timespan  \n",
       "0  - El artículo aborda un problema contingente y...  2010-07-05  \n",
       "1  El artículo presenta recomendaciones prácticas...  2010-07-05  \n",
       "2  - El tema es muy interesante y puede ser de mu...  2010-07-05  \n",
       "3  Se explica en forma ordenada y didáctica una e...  2010-07-05  \n",
       "4                                                     2010-07-05  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_es = reviews.query('lan == \"es\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      - El artículo aborda un problema contingente y...\n",
       "1      El artículo presenta recomendaciones prácticas...\n",
       "2      - El tema es muy interesante y puede ser de mu...\n",
       "3      Se explica en forma ordenada y didáctica una e...\n",
       "4                                                       \n",
       "                             ...                        \n",
       "400    El trabajo pretende ofrecer una visión del uso...\n",
       "401    El paper está bien escrito y de fácil lectura....\n",
       "402    Observación de fondo:  No se presenta un ejemp...\n",
       "403    Se propone un procedimiento para elaborar máqu...\n",
       "404    El artículo describe básicamente los component...\n",
       "Name: text, Length: 388, dtype: object"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_es.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "text_train, text_test, y_train, y_test = train_test_split(reviews_es.text, reviews_es.evaluation, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 65826\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# vectorizer = CountVectorizer(min_df=1,token_pattern=r'(?u)\\b[^_\\d\\W][^_\\d\\W]+\\b').fit(text_train)\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 3), token_pattern=r'(?u)\\b[^_\\d\\W][^_\\d\\W]+\\b').fit(text_train)\n",
    "print(\"Vocabulary size: {}\".format(len(vectorizer.vocabulary_)))\n",
    "# print(\"Vocabulary content:\\n {}\".format(vectorizer.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: <291x65826 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 109348 stored elements in Compressed Sparse Row format>\n"
     ]
    }
   ],
   "source": [
    "X_train = vectorizer.transform(text_train)\n",
    "print(\"X_train: {}\".format(repr(X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean cross-validation accuracy: 0.37\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "scores = cross_val_score(LogisticRegression(max_iter=100000), X_train, y_train, cv=5)\n",
    "print(\"Mean cross-validation accuracy: {:.2f}\".format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: 0.36\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(max_iter=100000)\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "X_test = vectorizer.transform(text_test)\n",
    "\n",
    "print(\"Test score: {:.2f}\".format(lr.score(X_test, y_test)))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
