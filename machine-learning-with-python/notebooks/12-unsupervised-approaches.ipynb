{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning with Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_files\n",
    "\n",
    "reviews_train = load_files(\"../assets/imdb/train/\")\n",
    "text_train, y_train = reviews_train.data, reviews_train.target\n",
    "text_train = [doc.replace(b\"<br />\", b\" \") for doc in text_train]\n",
    "\n",
    "reviews_test = load_files(\"../assets/imdb/test/\")\n",
    "text_test, y_test = reviews_test.data, reviews_test.target\n",
    "text_test = [doc.replace(b\"<br />\", b\" \") for doc in text_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Unsupervised Approaches\n",
    "\n",
    "There are a number of unsupervised approaches in NLP which can give us useful information about the substructure of a corpus, or suggest relevance rankings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Modelling\n",
    "\n",
    "The idea of *topic modelling* is to assign each document to one or more topic classes. For example, news data can usually be assigned to *politics*, *sport*, *finance*, etc.\n",
    "\n",
    "The *Latent Dirichlet Allocation* (LDA) approach models each document as a weighted *mixture* of topics. Topics will be derived from groups of words that tend to occur together. For example, \"MP\", \"minister\" and \"vote\" will tend to be found together, while \"goal\", \"team\" and \"score\" will tend to be found together. Because this is an unsupervised approach, the topics obtained may not coincide with our understanding of the relevant groups withing the corpus. However, LDA can still be a useful way to reduce the dimensionality of the data for further exploration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vect = CountVectorizer(max_features=10000, max_df=.15)\n",
    "X = vect.fit_transform(text_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "lda = LatentDirichletAllocation(n_components=10,learning_method=\"batch\",\n",
    "                                max_iter=25, random_state=0)\n",
    "# We build the model and transform the data in one step\n",
    "# Computing transform takes some time,\n",
    "# and we can save time by doing both at once\n",
    "document_topics = lda.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lda.components_.shape: (10, 10000)\n"
     ]
    }
   ],
   "source": [
    "print(\"lda.components_.shape: {}\".format(lda.components_.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each topic (a row in the components_), sort the features (ascending).\n",
    "# Invert rows with [:, ::-1] to make sorting descending\n",
    "sorting = np.argsort(lda.components_, axis=1)[:, ::-1]\n",
    "# get the feature names from the vectorizer:\n",
    "feature_names = np.array(vect.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the top-weighted features for the ten topics identified:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic 0 ['between' 'young' 'family' 'real' 'performance']\n",
      "topic 1 ['war' 'world' 'us' 'our' 'american']\n",
      "topic 2 ['funny' 'worst' 'comedy' 'thing' 'guy']\n",
      "topic 3 ['show' 'series' 'episode' 'tv' 'episodes']\n",
      "topic 4 ['didn' 'saw' 'am' 'thought' 'years']\n",
      "topic 5 ['horror' 'action' 'effects' 'budget' 'nothing']\n",
      "topic 6 ['kids' 'action' 'animation' 'game' 'fun']\n",
      "topic 7 ['cast' 'role' 'john' 'version' 'novel']\n",
      "topic 8 ['performance' 'role' 'john' 'actor' 'oscar']\n",
      "topic 9 ['house' 'woman' 'gets' 'killer' 'girl']\n"
     ]
    }
   ],
   "source": [
    "for topic_id in range(len(sorting)):\n",
    "    features = feature_names[sorting[topic_id]]\n",
    "    print( \"topic\", topic_id, features[0:5] )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use a larger number of topics to get finer resolution on the substructure of the corpus.\n",
    "\n",
    "If few training data are available, transforming into the lower-dimensional LDA feature space may be a helpful preprocessing step for supervised learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'In late 1800s San Francisco, poor well-dressed Errol Flynn (as James J. Corbett) works at a bank, and enjoys attending local \"fights\" (boxing) with co-worker and drinking buddy Jack Carson (as Walter Lowrie).\\n'\n",
      "b'\"The Ex-Mrs. Bradford\" (1936), starring Thin Man series star William Powell (this film was released the same year as the second Thin Man film, \"After The Thin Man,\" comes very close to duplicating the fun and style of the Thin Man films, but it nonetheless misses.\\n'\n",
      "b\"The year is 1896.Jeff Webster (James Stewart) doesn't like people.\\n\"\n",
      "b'Josef Von Sternberg directs this magnificent silent film about silent Hollywood and the former Imperial General to the Czar of Russia who has found himself there. Emil Jannings won a well-deserved Oscar, in part, for his role as the general who ironically is cast in a bit part in a silent picture as a Russian general.\\n'\n",
      "b\"Sidney Stratton is having trouble maintaining jobs at various textile mills mainly because of his experimentation in the textile laboratories. Stratton's experimenting on a formula for a new fabric which would create the ultimate fabric, one that never gets dirty, never wrinkles, or wears out.\\n\"\n"
     ]
    }
   ],
   "source": [
    "# sort by weight of \"horror\" topic 7\n",
    "horror = np.argsort(document_topics[:, 7])[::-1]\n",
    "# print the five documents where the topic is most important\n",
    "for i in horror[:5]:\n",
    "    # show first two sentences\n",
    "    print(b\".\".join(text_train[i].split(b\".\")[:2]) + b\".\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding Similar Documents\n",
    "\n",
    "We can imagine using a distance metric within the *bag of words*, *n-grams* or LDA feature space to find documents that are \"similar\" to a given input document.\n",
    "\n",
    "However, these features are not good at capturing the *semantic* similarity between documents. Consider\n",
    "\n",
    "* The cat sat on the mat\n",
    "* The feline rested on the rug\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`word2vec` is a very popular approach to this problem that is based on neural networks. Using a large unannotated plain text corpus, `word2vec` constructs a *continuous* feature space with a predefined dimensionality, within which individual words are located. Words with similar semantic meaning are located close together in this feature space. More remarkably, certain linear relationships can be identified:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vec(“king”) - vec(“man”) + vec(“woman”) =~ vec(“queen”)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `gensim` package to explore some of the possibilities of this approach.\n",
    "\n",
    "https://radimrehurek.com/gensim/index.html\n",
    "\n",
    "Firstly, we will download a pre-trained model with 100 dimensions, trained on 400,000 Wikipedia articles from 2014."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 128.1/128.1MB downloaded\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "model = api.load(\"glove-wiki-gigaword-100\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.keyedvectors.KeyedVectors at 0x7f170f11dd80>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model gives the vector representation of any word from the corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.59242  ,  0.64671  , -0.17863  , -0.32955  , -0.33752  ,\n",
       "       -0.3927   ,  1.5196   , -0.51104  ,  0.11168  , -0.36983  ,\n",
       "       -0.99754  ,  0.097873 , -0.057832 ,  0.72555  , -0.071104 ,\n",
       "        0.57666  ,  0.23989  , -0.090268 , -0.15606  , -0.077584 ,\n",
       "       -0.77263  ,  0.53222  ,  0.91333  , -0.54726  ,  0.2099   ,\n",
       "        1.0181   ,  0.068781 , -0.073462 , -0.62079  , -0.13303  ,\n",
       "       -0.29637  ,  0.70528  ,  0.72128  , -0.73807  , -0.12144  ,\n",
       "        0.74609  , -0.062772 ,  0.12818  ,  0.56983  , -0.60158  ,\n",
       "        0.09011  , -1.3489   , -0.2133   , -0.80586  ,  0.33789  ,\n",
       "        0.14686  ,  0.057557 ,  0.12637  , -0.41219  , -0.20943  ,\n",
       "       -0.87134  , -0.1932   ,  0.15099  ,  0.57204  , -1.0344   ,\n",
       "       -0.16244  ,  0.15535  ,  0.51579  ,  0.55834  ,  0.2793   ,\n",
       "        0.77384  ,  0.91842  ,  0.13026  , -0.19378  ,  0.86953  ,\n",
       "       -0.81084  ,  0.26939  , -0.39627  ,  0.42148  , -0.56352  ,\n",
       "       -0.19354  , -0.0036075,  0.40456  ,  0.14934  , -0.52379  ,\n",
       "        0.67058  , -0.54152  ,  0.1367   ,  0.47423  ,  1.3548   ,\n",
       "        0.24095  , -0.49564  , -0.60962  ,  0.2795   , -0.50251  ,\n",
       "       -0.31224  ,  0.036952 , -0.2661   ,  0.042465 ,  0.11073  ,\n",
       "        0.63545  ,  0.66698  , -0.37126  ,  0.31704  , -0.97271  ,\n",
       "       -1.3302   ,  0.45629  , -0.22584  ,  0.51449  , -0.2066   ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['potato']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['potato'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model can compute similarities to other words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('potatoes', 0.778667151927948),\n",
       " ('peanut', 0.7455057501792908),\n",
       " ('tomato', 0.745377779006958),\n",
       " ('bread', 0.736748218536377),\n",
       " ('cheese', 0.7155595421791077),\n",
       " ('baked', 0.7084025740623474),\n",
       " ('pumpkin', 0.7009469866752625),\n",
       " ('fried', 0.7007490396499634),\n",
       " ('cabbage', 0.6885643005371094),\n",
       " ('bean', 0.6814960837364197)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"potato\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By embedding documents into this space, document similarity can be computed."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
